# -*- coding: utf-8 -*-
"""final_results_gender_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/MITESHPUTHRANNEU/Speech-Emotion-Analyzer/blob/master/final_results_gender_test.ipynb


## Importing the required libraries
"""


import librosa

import librosa.display

import numpy as np

import matplotlib.pyplot as plt, IPython.display as ipd
import matplotlib as mpl
import itertools
from itertools import cycle
import cv2


import tensorflow as tf

import os
import pandas as pd
import glob 
import scipy.io.wavfile
from scipy import interp
import sys
import json
import random

import soundfile
import pickle
import tqdm
import seaborn as sns


from tensorflow import keras

from matplotlib.pyplot import specgram
from matplotlib import cm
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import model_from_json
from tensorflow.keras.layers import LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import StandardScaler
from tensorflow.keras import Input
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras.layers import Add
from tensorflow.keras.layers import Concatenate




from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization, MaxPooling2D,GlobalAveragePooling2D,Input
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import optimizers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers import RMSprop

#from tensorflow.python.keras.applications.resnet import ResNet50
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense

from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import CSVLogger
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix

from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import LabelBinarizer
from sklearn.preprocessing import MultiLabelBinarizer

from sklearn.utils import shuffle

from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier
from itertools import cycle

from datetime import datetime

mylist= os.listdir('RawData/')


data, sampling_rate = librosa.load('RawData/01-01-01-01-01-01-01.wav')

print(type(data), type(sampling_rate))

print('data.shape',data.shape)
print('sampling_rate',sampling_rate)



plt.figure(figsize=(14, 5))
librosa.display.waveplot(data, sr=sampling_rate)

plt.show()




# stft

plt.figure(figsize=(15, 5))
librosa.display.waveplot(data, sr=sampling_rate)
sr,x = scipy.io.wavfile.read('RawData/01-01-01-01-01-01-01.wav')

print('sr',sr)

print('x',x.shape)

hop_length = 441
n_fft = 512
D = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)

#float(hop_length)/sr # units of seconds
#float(n_fft)/sr  # units of seconds

print('D',D.shape)


print('\n')

S = librosa.amplitude_to_db(abs(D))

print('S',S)
print(S.shape)

plt.figure(figsize=(15, 5))
librosa.display.specshow(S, sr=sr, hop_length=hop_length, x_axis='time', y_axis='linear')
plt.colorbar(format='%+2.0f dB')

plt.show()
print('\n stft.shape')
print(S.shape)


df = pd.DataFrame(columns=['feature'])
bookmark = 0

hop_length = 441
n_fft = 512

feeling_list = []

for index,y in enumerate(mylist):
   
    if mylist[index][6:-16]!='01' and mylist[index][6:-16]!='07' and mylist[index][6:-16]!='08' and mylist[index][:2]!='su' and                mylist[index][:1]!='n' and mylist[index][:1]!='d':
           X, sample_rate = librosa.load('RawData/'+y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)
           # STFT
           D  = librosa.stft(X, n_fft=n_fft, hop_length=hop_length)
           
           S = librosa.amplitude_to_db(abs(D))
          
           #if S.shape != (257, 251) :
             # print("Audio file: ", y)
              #input('')
             
           if S.shape != (257, 251):
               continue
           feature = S
           df.loc[bookmark] = [feature]
           bookmark=bookmark+1
           item = mylist[index]
           if item[6:-16]=='02' and int(item[18:-4])%2==0:
               feeling_list.append('female_calm')
           elif item[6:-16]=='02' and int(item[18:-4])%2==1:
               feeling_list.append('male_calm')
           elif item[6:-16]=='03' and int(item[18:-4])%2==0:
               feeling_list.append('female_happy')
           elif item[6:-16]=='03' and int(item[18:-4])%2==1:
               feeling_list.append('male_happy')
           elif item[6:-16]=='04' and int(item[18:-4])%2==0:
               feeling_list.append('female_sad')
           elif item[6:-16]=='04' and int(item[18:-4])%2==1:
               feeling_list.append('male_sad')
           elif item[6:-16]=='05' and int(item[18:-4])%2==0:
               feeling_list.append('female_angry')
           elif item[6:-16]=='05' and int(item[18:-4])%2==1:
               feeling_list.append('male_angry')
           elif item[6:-16]=='06' and int(item[18:-4])%2==0:
               feeling_list.append('female_fearful')
           elif item[6:-16]=='06' and int(item[18:-4])%2==1:
               feeling_list.append('male_fearful')
           elif item[:1]=='a':
               feeling_list.append('male_angry')
           elif item[:1]=='f':
               feeling_list.append('male_fearful')
           elif item[:1]=='h':
               feeling_list.append('male_happy')
           #elif item[:1]=='n':
           #feeling_list.append('neutral')
           elif item[:2]=='sa':
               feeling_list.append('male_sad')

labels = pd.DataFrame(feeling_list)


print(df.shape,labels.shape)

x = np.array(df.feature.tolist())

print(x.shape)

y=labels.to_numpy().ravel()
print('y.shape',y.shape)

#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state = 42)

skf = StratifiedKFold(n_splits = 10)
skf.get_n_splits(x, y)

print(skf)

StratifiedKFold(n_splits = 10, random_state=None, shuffle=False)

for train_index, test_index in skf.split(x, y):
	print("TRAIN:", train_index, "TEST:", test_index)
	x_train, x_test = x[train_index], x[test_index]     
	y_train, y_test = y[train_index], y[test_index]


print('x_train', x_train.shape)
print('x_test' , x_test.shape)
print('y_train', y_train.shape)
print('y_test' , y_test.shape)


lb = LabelEncoder()

y_train = to_categorical(lb.fit_transform(y_train))
y_test = to_categorical(lb.fit_transform(y_test))

print('y_train',y_train.shape)

num_rows     = 257
num_columns  = 251
num_channels = 1

leaky_relu_alpha = 0.1
#filter_size = 2

x_train = x_train.reshape(x_train.shape[0],num_rows,num_columns,num_channels)
x_test  = x_test.reshape(x_test.shape[0],num_rows,num_columns,num_channels)


print('new_train',x_train.shape)

num_labels = y_train.shape[1]

def make_dilated_network(num_rows, num_columns, num_channels):
    input_data = Input(shape=(num_rows, num_columns, num_channels))
    x = Conv2D(filters=8, kernel_size=7)(input_data)
    x = Conv2D(filters=16, kernel_size=5, padding = 'same')(x)
    x = MaxPooling2D(pool_size=2)(x)
    x = BatchNormalization()(x)
    x = LeakyReLU()(x)

    x_1 = Conv2D(filters=32, kernel_size=5, padding = 'same')(x)
    x_1 = MaxPooling2D(pool_size=2)(x_1)
    x_1 = BatchNormalization()(x_1)
    x_2 = Conv2D(filters=32, kernel_size=5, padding="same", dilation_rate=2)(x)
    x_2 = MaxPooling2D(pool_size=2)(x_2)
    x_2 = BatchNormalization()(x_2)
    x = Add()([x_1, x_2])
    x = LeakyReLU()(x)

    x_1 = Conv2D(filters=64, kernel_size=5, padding = 'same')(x)
    x_1 = MaxPooling2D(pool_size=2)(x_1)
    x_1 = BatchNormalization()(x_1)
    x_2 = Conv2D(filters=64, kernel_size=5, padding="same", dilation_rate=2)(x)
    x_2 = MaxPooling2D(pool_size=2)(x_2)
    x_2 = BatchNormalization()(x_2)
    x = Add()([x_1, x_2])
    x = LeakyReLU()(x)
    
    x_1 = Conv2D(filters=64, kernel_size=5, padding = 'same')(x)
    x_1 = MaxPooling2D(pool_size=2)(x_1)
    x_1 = BatchNormalization()(x_1)
    x_2 = Conv2D(filters=64, kernel_size=5, padding="same", dilation_rate=2)(x)
    x_2 = MaxPooling2D(pool_size=2)(x_2)
    x_2 = BatchNormalization()(x_2)
    x = Add()([x_1, x_2])
    x = LeakyReLU()(x)

    x = Flatten()(x)
    x = Dense(num_labels, activation='softmax')(x)

    model = tf.keras.Model(inputs=input_data, outputs=x, name="Dilated_STFT")
    return model

# Construct model 
model = Sequential()
model = make_dilated_network(num_rows, num_columns, num_channels)

rms = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)
model.summary()

# defining the parameters for RMSprop (I used the keras defaults here)
model.compile(loss='categorical_crossentropy', metrics=['categorical_accuracy'],optimizer=rms)

# Display model architecture summary 
print(model)


# Calculate pre-training accuracy 
score = model.evaluate(x_test, y_test, verbose=1)
accuracy = 100*score[1]

print("Pre-training accuracy: %.4f%%" % accuracy)

#callback = tf.keras.callbacks.EarlyStopping(verbose=1, patience=10,min_delta=0.05)
csv_logger = CSVLogger('log.csv', append=True, separator=';')
history = model.fit(x_train, y_train, batch_size=16, epochs=200, validation_data=(x_test,y_test),callbacks=[csv_logger])


# Evaluating the model on the training and testing set
score = model.evaluate(x_train, y_train, verbose=0)
print("Training Accuracy: ", score[1])

score2 = model.evaluate(x_test, y_test, verbose=0)
print("Testing Accuracy: ", score2[1])


accuracy       =  history.history['categorical_accuracy']
val_accuracy   =  history.history['val_categorical_accuracy']

loss           =  history.history['loss']
val_loss       =  history.history['val_loss']

epochs = range(1, len(accuracy) + 1)

plt.plot(epochs, accuracy, 'bo', label='Training accuracy')
plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')
plt.title('CNN Accuracy')
plt.legend()
plt.savefig('CNN Accuracy')

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('CNN Loss')
plt.legend()
plt.savefig('CNN Loss')
plt.show()


target_names=['female_calm','male_calm','female_happy','male_happy','female_sad','male_sad','female_angry','male_angry','female_fearful','male_fearful']

y_pred = model.predict(x_test)

print('y_pred',y_pred)
print('y_pred.shape',y_pred.shape)

y_true = np.argmax(y_test, axis=1)

y_pred_classes = np.argmax(y_pred, axis=1)


#y_pred_classes = np.argmax(y_pred, axis=1)

#print(classification_report(y_true, y_pred, target_names=target_names))

print('\n', classification_report(np.where(y_test > 0)[1], np.argmax(y_pred, axis=1),
                                  target_names=target_names))

'''
print(data = confusion_matrix(y_true, y_pred))

# Plot non-normalized confusion matrix

df_cm = pd.DataFrame(data, columns=np.unique(target_names), index = np.unique(target_names))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (10,7))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, cmap="Blues", annot=True,annot_kws={"size": 16})

plt.savefig("confusion_matrix.png")
plt.show()
'''
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    plt.figure(10)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    y = np.repeat(np.arange(0, 10), 75)
    plt.xlim(-0.5, len(np.unique(y)) - 0.5)  # ADD THIS LINE
    plt.ylim(len(np.unique(y)) - 0.5, -0.5)  # ADD THIS LINE
    plt.savefig("confusion_matrix_big.png")

cm = confusion_matrix(y_true, y_pred_classes)

plot_confusion_matrix(cm, classes=target_names)


print("Training Accuracy: ", score[1])

print("Testing Accuracy: ", score2[1])

